{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr[\\mathbf{X}, Y] = Pr[Y]\\prod^K_{j=1}Pr[X_j|Y]$$\n",
    "\n",
    "対数尤度は\n",
    "$$\\mathcal{L}(\\mathcal{D};\\{Pr[y]\\},\\{Pr[x_j|y]\\})=\\sum_{(\\mathbf{x}_i,y_i)\\in\\mathcal{D}}\\ln{Pr[\\mathbf{x}_i|y_i]}$$\n",
    "\n",
    "予測するときは\n",
    "$$\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\\\\\n",
    "\\begin{eqnarray*}\n",
    "\\hat{y}&=&\\argmax_y Pr[y|\\mathbf{x}^{new}]\\\\\n",
    "&=&\\argmax_y \\frac{Pr[y]Pr[\\mathbf{x}^{new}|y]}{\\sum_y{Pr[y]Pr[\\mathbf{x}^{new}|y]}}\\\\\n",
    "&=&\\argmax_y Pr[y]Pr[\\mathbf{x}^{new}|y]\\\\\n",
    "&=&\\argmax_y \\biggl(Pr[y]\\prod_j{Pr[x^{new}_j|y]}\\biggr)\\\\\n",
    "&=&\\argmax_y \\biggl(\\log{Pr[y]}+\\sum_j{\\log{Pr[x^{new}_j|y]}}\\biggr)\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仕様設計\n",
    "- データに依存しないアルゴリズムのパラメータは、クラスのコンストラクタの引数で指定する\n",
    "- 学習はfit()メソッドで行う。訓練データとデータに依存したパラメータを、このメソッドの引数で指定する\n",
    "- 予測はpredict()メソッドで行う。新規の入力データを、このメソッドの引数で指定する\n",
    "- モデルのデータの当てはめの良さの評価は、score()メソッドで行う。評価対象のデータを、このメソッドの引数で指定する\n",
    "- 次元削除などのデータ変換は、transform()メソッドで行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayes1(object):\n",
    "    def __init__(self):\n",
    "        self.pY_ = None\n",
    "        self.pXgY_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fitting Model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like, shape=(n_samples, n_features), dtype=int\n",
    "            feature values of training samples\n",
    "        y: array_like, shape=(n_samples), dtype=int\n",
    "            class labels of training samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # constants\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = 2\n",
    "        n_fvalues = 2\n",
    "        \n",
    "        # check the size of y\n",
    "        if n_samples != len(y):\n",
    "            raise ValueError('Mismatched number of samples.')\n",
    "        \n",
    "        # count up n[yi=y]\n",
    "        nY = np.zeros(n_classes, dtype=np.int)\n",
    "        for i in xrange(n_samples):\n",
    "            nY[y[i]] += 1\n",
    "        \n",
    "        # calc pY_\n",
    "        self.pY_ = np.empty(n_classes, dtype=np.float)\n",
    "        for i in xrange(n_classes):\n",
    "            self.pY_[i] = nY[i] / np.float(n_samples)\n",
    "    \n",
    "        # count up n[x_ij=xj, yi=y]\n",
    "        nXY = np.zeros((n_features, n_fvalues, n_classes), dtype=np.int)\n",
    "        for i in xrange(n_samples):\n",
    "            for j in xrange(n_features):\n",
    "                nXY[j, X[i, j], y[i]] += 1\n",
    "                \n",
    "        # calc pXgY_\n",
    "        self.pXgY_ = np.empty((n_features, n_fvalues, n_classes), dtype=np.float)\n",
    "        for j in xrange(n_features):\n",
    "            for xi in xrange(n_fvalues):\n",
    "                for yi in xrange(n_classes):\n",
    "                    self.pXgY_[j, xi, yi] = nXY[j, xi, yi] / np.float(nY[yi])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like, shape=(n_samples, n_features), dtype=int\n",
    "            feature values of unseen samples\n",
    "            \n",
    "        Returns\n",
    "        -----\n",
    "        y: array_like, shape=(n_samples), dtype=int\n",
    "            predict class labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # constants\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # memory for return values\n",
    "        y = np.empty(n_samples, dtype=np.int)\n",
    "        \n",
    "        # for each feature in X\n",
    "        for i, xi in enumerate(X):\n",
    "            \n",
    "            # calc join probablity\n",
    "            logpXY = np.log(self.pY_) + \\\n",
    "                np.sum(np.log(self.pXgY_[np.arange(n_features),xi,:]), axis=0)\n",
    "            \n",
    "            # predict class\n",
    "            y[i] = np.argmax(logpXY)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('vote_filled.tsv', dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr = NaiveBayes1()\n",
    "clr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_y = clr.predict(X[:10, :])\n",
    "for i in xrange(10):\n",
    "    print i, y[i], predict_y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスの再編成\n",
    "予測メソッドなどの共通部分を含む抽象クラスを作成し、その抽象クラスを継承した下位クラスを実装していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class BaseBinaryNaiveBayes(object):\n",
    "    \"\"\"\n",
    "    Abstract Class for Naive Bayes whose classes and features are binary.\n",
    "    \"\"\"\n",
    "    \n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pY_ = None\n",
    "        self.pXgY_ = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Abstract method for fitting model\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        `pY_` : array_like, shape=(n_classes), dtype=float\n",
    "            pmf of a class\n",
    "        `pXgY_` : array_like, shape(n_features, n_classes, n_fvalues), dtype=float\n",
    "            pmf of feature values given a class\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like, shape=(n_samples, n_features), dtype=int\n",
    "            feature values of unseen samples\n",
    "            \n",
    "        Returns\n",
    "        -----\n",
    "        y: array_like, shape=(n_samples), dtype=int\n",
    "            predict class labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # constants\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # memory for return values\n",
    "        y = np.empty(n_samples, dtype=np.int)\n",
    "        \n",
    "        # for each feature in X\n",
    "        for i, xi in enumerate(X):\n",
    "            \n",
    "            # calc join probablity\n",
    "            logpXY = np.log(self.pY_) + \\\n",
    "                np.sum(np.log(self.pXgY_[np.arange(n_features),xi,:]), axis=0)\n",
    "            \n",
    "            # predict class\n",
    "            y[i] = np.argmax(logpXY)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class NaiveBayes1(BaseBinaryNaiveBayes):\n",
    "    \"\"\"\n",
    "    Naive Bayes class (1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NaiveBayes1, self).__init__()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fitting Model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like, shape=(n_samples, n_features), dtype=int\n",
    "            feature values of training samples\n",
    "        y: array_like, shape=(n_samples), dtype=int\n",
    "            class labels of training samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # constants\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = 2\n",
    "        n_fvalues = 2\n",
    "        \n",
    "        # check the size of y\n",
    "        if n_samples != len(y):\n",
    "            raise ValueError('Mismatched number of samples.')\n",
    "        \n",
    "        # count up n[yi=y]\n",
    "        nY = np.zeros(n_classes, dtype=np.int)\n",
    "        for i in xrange(n_samples):\n",
    "            nY[y[i]] += 1\n",
    "        \n",
    "        # calc pY_\n",
    "        self.pY_ = np.empty(n_classes, dtype=np.float)\n",
    "        for i in xrange(n_classes):\n",
    "            self.pY_[i] = nY[i] / np.float(n_samples)\n",
    "    \n",
    "        # count up n[x_ij=xj, yi=y]\n",
    "        nXY = np.zeros((n_features, n_fvalues, n_classes), dtype=np.int)\n",
    "        for i in xrange(n_samples):\n",
    "            for j in xrange(n_features):\n",
    "                nXY[j, X[i, j], y[i]] += 1\n",
    "                \n",
    "        # calc pXgY_\n",
    "        self.pXgY_ = np.empty((n_features, n_fvalues, n_classes), dtype=np.float)\n",
    "        for j in xrange(n_features):\n",
    "            for xi in xrange(n_fvalues):\n",
    "                for yi in xrange(n_classes):\n",
    "                    self.pXgY_[j, xi, yi] = nXY[j, xi, yi] / np.float(nY[yi])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes (2)\n",
    "\n",
    "### 特徴分布の学習\n",
    "\n",
    "次元 | ループ変数 | 大きさ | 意味\n",
    "---- | ---- | ---- | ----\n",
    "0 | i | n_samples | 事例\n",
    "1 | j | n_features | 特徴\n",
    "2 | xi | n_fvalues | 特徴値\n",
    "3 | yi | n_classes | クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nbayes1b import BaseBinaryNaiveBayes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayes2(BaseBinaryNaiveBayes):\n",
    "    \"\"\"\n",
    "    Naive Bayes (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NaiveBayes2, self).__init__()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fitting Model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array_like, shape=(n_samples, n_features), dtype=int\n",
    "            feature values of training samples\n",
    "        y: array_like, shape=(n_samples), dtype=int\n",
    "            class labels of training samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # constants\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = 2\n",
    "        n_fvalues = 2\n",
    "        \n",
    "        # check the size of y\n",
    "        if n_samples != len(y):\n",
    "            raise ValueError('Missmatched number of values.')\n",
    "        \n",
    "        # count up n[yi=y]\n",
    "        nY = np.sum(y[:, np.newaxis] == np.arange(n_classes)[np.newaxis, :], axis=0)\n",
    "        \n",
    "        # calc pY_\n",
    "        self.pY_ = np.true_divide(nY, n_samples)\n",
    "        \n",
    "        # count up n[x_ij=xj, yi=y]\n",
    "        ary_xi = np.arange(n_fvalues)[np.newaxis, np.newaxis, :, np.newaxis]\n",
    "        ary_yi = np.arange(n_classes)[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "        ary_y = y[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "        ary_X = X[:, :, np.newaxis, np.newaxis]\n",
    "\n",
    "        nXY = np.sum(np.logical_and(ary_X == ary_xi, ary_y == ary_yi), axis=0)\n",
    "        \n",
    "        # calc pXgY_\n",
    "        self.pXgY_ = np.true_divide(nXY, nY[np.newaxis, np.newaxis, :])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nbayes2 import *\n",
    "data = np.genfromtxt(\"vote_filled.tsv\", dtype=np.int)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "clr1 = NaiveBayes1()\n",
    "clr2 = NaiveBayes2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.68 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100 loops, best of 3: 6.76 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit clr1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 529 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit clr2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "1 1 1\n",
      "2 0 0\n",
      "3 0 0\n",
      "4 0 0\n",
      "5 0 0\n",
      "6 0 1\n",
      "7 1 1\n",
      "8 1 1\n",
      "9 0 0\n"
     ]
    }
   ],
   "source": [
    "clr = NaiveBayes2()\n",
    "clr.fit(X, y)\n",
    "\n",
    "predict_y = clr.predict(X[:10, :])\n",
    "for i in xrange(10):\n",
    "    print i, y[i], predict_y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
